{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-dimensional search methods play an important role in multi-dimensional optimization problems. In particular, iterative algorithms for solving such optimization problems typically involve a line-search at every iteration. To be specific, let $f:\\mathbf{R}^n \\to \\mathbf{R}$ be a function that we wish to minimize. \n",
    "\n",
    "A common approach to optimization is to incrementally improve upon $\\mathbf{x}$ by taking a step that minimizes the objective value based on a local model. The local model may be obtained, from a first-order or second-order Taylor's approximation. Optimization algorithms that follow this general approach are referred to as *descent direction methods*. They start with an initial point $\\mathbf{x}^{(1)}$, then generate a sequence $\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\mathbf{x}^{(3)}, \\ldots, \\mathbf{x}^{(k)},\\ldots$, called iterates to converge to a local minimum. \n",
    "\n",
    "The iterative descent direction algorithm involves the following steps:\n",
    "\n",
    "1. Check whether $\\mathbf{x}^{(k)}$ satisfies the termination procedure. If it does, terminate; otherwise proceed to the next step.\n",
    "\n",
    "2. Determine the *descent direction* $\\mathbf{d}^{(k)}$ using local information such as the gradient or Hessian. Some algorithms assume the magnitude/length of the direction vector as $1$, $\\lvert \\lvert \\mathbf{d}^{(k)} \\rvert \\rvert = 1$, but others do not.\n",
    "\n",
    "3. Determine the step size or the learning rate $\\alpha^{(k)}$. Some algorithms attempt to optimize the step-size so that the step maximally decreases $f$.\n",
    "\n",
    "4. Compute the next point according to:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x}^{(k+1)} \\leftarrow \\mathbf{x}^{k} + \\alpha^{(k)}\\mathbf{d}^{(k)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the moment assume that we have chosen a descent direction $\\mathbf{d}^{(k)}$ perhaps using one of the methods discussed at length in the subsequent notebooks. We need to choose the step factor $\\alpha$ to obtain our next point.One approach is to use *exact line search*, which minimizes the function $\\phi_k(\\alpha)$ of a single variable $\\alpha$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\phi_k(\\alpha) = f(\\mathbf{x}^{(k)} + \\alpha \\mathbf{d}^{(k)})\n",
    "\\end{align*}\n",
    "\n",
    "Line search is a univariate optimization problem. We can therefore, apply the univariate optimization method of our choice. To inform the search, we can use the derivative of the line search objective, which is simply the directional derivative along $\\mathbf{d}$ at $\\mathbf{x} + \\alpha \\mathbf{d}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bracket_minimum (generic function with 4 methods)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using NBInclude\n",
    "@nbinclude(\"bracket_minimum.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bisection (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bisection(f, a₀, b₀, ϵ)\n",
    "\n",
    "    function D(f,a)\n",
    "        # Approximate the first derivative using central differences\n",
    "        h = 0.001\n",
    "        return (f(a + h) - f(a - h))/(2 * h)\n",
    "    end\n",
    "\n",
    "    a = a₀\n",
    "    b = b₀\n",
    "\n",
    "    while((b - a) > ϵ)\n",
    "        c = (a + b)/2.0\n",
    "\n",
    "        if D(f,c) > 0\n",
    "            b = c\n",
    "        else\n",
    "            a = c\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # For the purposes of line search, we will return the center of the \n",
    "    # bracketing interval and not the boundary points.\n",
    "    return (a + b)/2.0  \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function line_search(f, x, d)\n",
    "    objective(α) = f(x .+ α .* d)\n",
    "    a, b = bracket_minimum(objective)\n",
    "    α = bisection(objective, a, b, 1e-5)\n",
    "    return α, x .+ α .* d\n",
    "end\n",
    "\n",
    "f(x) = sin(x[1] * x[2]) + exp(x[2] + x[3]) - x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.1270471191406246, [1.0, -1.1270471191406246, -0.1270471191406246])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 2, 3]\n",
    "d = [0, -1, -1]\n",
    "\n",
    "α, x_min = line_search(f, x, d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate line search\n",
    "\n",
    "It is often more computationally efficient to perform more iterations of a descent method than to do exact line search, especially if the function and derivative calculations are expensive. Many of the methods discussed so far can benefit from using *approximate line search* by choosing a suitable step-size with a small number of evaluations. Since descent methods must descend, a step size $\\alpha$ may be suitable if it causes a decrease in the objective function value. However, a variety of other conditions may be enforced to encourage faster convergence. \n",
    "\n",
    "The condition for sufficient decrease requires that the step size cause a sufficient decrease in the objective function value. \n",
    "\n",
    "\\begin{align*}\n",
    "f(\\mathbf{x}^{(k+1)}) \\le f(\\mathbf{x}^{(k)}) + \\beta \\alpha \\nabla f(\\mathbf{x}^{(k)}) \\cdot \\mathbf{d}^{(k)}\n",
    "\\end{align*}\n",
    "\n",
    "with $\\beta \\in [0,1]$ often set to $\\beta = 10^{-4}$. The univariate function $l(\\alpha) = f(\\mathbf{x}^{(k)}) + \\beta \\alpha \\nabla f(\\mathbf{x}^{(k)}) \\cdot \\mathbf{d}^{(k)}$ is called the line of sufficient decrease. \n",
    "\n",
    "If $\\mathbf{d}$ is a valid descent direction, then there must exist a sufficiently small step size that satisfies the sufficient decrease condition. We can thus start with a large step size and decrease it by a constant reduction factor until the sufficient decrease condition (*Armijo condition*) is satisfied. This algorithm is known as *backtracking line search*. Backtracking line search is shown and implemented in the algorithm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backtracking_line_search (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Calculus\n",
    "using LinearAlgebra\n",
    "\n",
    "function backtracking_line_search(f, ∇f, x, d, α; ρ=0.5, β=1e-4)\n",
    "    ρ = 0.90\n",
    "    ϕ(α) = f(x .+ α .* d)\n",
    "    l(α) = f(x) + β * α * (∇f(x) ⋅ d)\n",
    "\n",
    "    while (ϕ(α) > l(α))  # First Wolfe condition\n",
    "        α = ρ * α\n",
    "    end\n",
    "    return α\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.82429536481"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x) = x[1]^2 + x[1]*x[2] + x[2]^2\n",
    "\n",
    "x = [1,2]    # kth iterate\n",
    "d = [-1,-1]  # descent direction vector\n",
    "α = 10       # maximum step-size\n",
    "β = 1e-4     # first Wolfe condition parameter\n",
    "ρ = 0.5      # Reduction factor\n",
    "\n",
    "∇f(x) = Calculus.finite_difference(f, x,:central)\n",
    "\n",
    "step_size = backtracking_line_search(f, ∇f, x, d, α, ρ=0.5, β=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
